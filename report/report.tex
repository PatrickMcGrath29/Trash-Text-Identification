%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[12pt]{article}

\usepackage{amstext}
\usepackage{array}
\usepackage{enumerate}
\usepackage{fontspec}
\usepackage{multicol}
\usepackage[square,sort,comma,numbers]{natbib}

\renewcommand{\baselinestretch}{1.2}
\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\thesection\arabic{subsection}}

\newcolumntype{L}{>{$}l<{$}}
\setmainfont{Linux Libertine O}

\title{Using Textual Features to Identify Document Reliability}
\author{Julien Cherry \and Patrick McGrath}
\date{April 26\textsuperscript{th}, 2019}

\begin{document}
	\maketitle
	\section{Problem Description}

	\section{Problem Formulation}

	\begin{itemize}
		\item Given the author, title, and text of an article, determine if the article is likely to be reliable or not
	\end{itemize}

	\begin{tabular}{l | L}
		Dataset        & D = \{\langle \text{title}_1, \text{author}_1, \text{text}_1, \text{reliability}_1 \rangle \ldots\} \\
		Training set   & T_a = randomSample_{0.5}(D) \\
		Testing set    & T_b = D \setminus T_a \\
		Classifier     & C = trainClassifier(T_a) \\
		Sample article & a_s \not\in D = \langle \text{title}_s, \text{author}_s, \text{text}_s \rangle \\
		Prediction     & p_s \in \{\text{reliable}, \text{unreliable}\} = predict(C, a_s) \\
	\end{tabular}

	\section{Methodology}

	To train our model, we used fake news data from data competiton platform Kaggle consisting of a 20,800-row CSV dataset for training and a 5,200-row CSV dataset for testing \cite{kaggle}. Each row represents a news article and contains a unique ID as well as the title, author, and text of the article. Additionally, the training file contains a reliability judgment label---1 for potentially unreliable, and 0 for reliable.

	Because we approached the problem with supervised learning and the testing dataset contains no reliability labels, we discarded it. We suspect that the reliability judments for this dataset are merely unavailable in order to prevent competitors from training on the testing dataset.

	We extracted and cleaned 18,285 articles from the training dataset with the pandas library \cite{pandas} and randomly withheld 50\% of the articles for use in a testing set. Using scikit-learn \cite{scikit-learn}, we vectorized the data into token--count tuples and weighted the features with TF--IDF to prioritize words that are more unique to a given document.

	We then used the data to train each of three classifiers: a naïve Bayes classifier, a stochastic gradient descent classifier, and a passive aggressive classifier. Once the classifiers were trained, we used the model to predict the reliability of articles in the testing dataset.

	\subsection{Passive Aggressive Classification}

	\begin{itemize}
		\item Online algorithm, streaming data
		\begin{itemize}
			\item Consume example, update classifier, discard example
		\end{itemize}
	\end{itemize}

	\section{Empirical Results}

	\begin{tabular}{r | l l l}
		                & Naïve Bayes & Stochastic Gradient Descent & Passive Aggressive \\
		\hline
		accuracy        & 70.67\%     & 94.76\%                     & 96.31\%            \\
		false positives & 2           & 211                         & 150                \\
		false negatives & 2680        & 268                         & 187                \\
		specificity     & 33.72\%     & 93.49\%                     & 96.08\%            \\
		precision       & 99.92\%     & 96.67\%                     & 96.12\%            \\
	\end{tabular}

	\textit{Specificity} is calculated as the number of correct negative predictions divided by the total number of negatives.

	\textit{Precision} is the number of correct positive results divided by the number of positive results predicted by the classifier.

	\section{Analysis}

	\begin{itemize}
		\item Objective: Identify misinformation
		\item Classification based on words in article
		\begin{itemize}
			\item Doesn’t account for context of words
		\end{itemize}
		\item Clear bias in dataset
		\begin{itemize}
			\item Overfitting?
		\end{itemize}
		\item Correlation !== Causation
	\end{itemize}

	\section{Discussion}
	\section{Future Work}

	Work in this area could be used to inform social media users whether linked articles are likely to be reliable or not. However, its implementation need be informed---Pennycook et al. found that labelling disputed articles with a warning causes unlabelled articles to appear more accurate. Crucially, they found that this \textit{Implied Truth Effect} is reversed when verified articles are also labelled \cite{pennycook}.

	\begin{itemize}
		\item Investigate what kinds of articles are misclassified
		\item Tweak specificity and precision of classifier
		\item Use n-grams, Word2Vec, Doc2Vec
		\begin{itemize}
			\item Improve context around words, instead of standalone words
		\end{itemize}
	\end{itemize}

	\section{Reflection}

	\begin{itemize}
		\item Have a more established definition of what misinformation is
		\item Opportunity to debug classification process
		\begin{itemize}
			\item Identify improvements
			\item Adjust features
		\end{itemize}
	\end{itemize}

	\bibliographystyle{plainnat}
	\bibliography{references}
\end{document}
