%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[12pt]{article}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{array}
\usepackage{enumerate}
\usepackage{fontspec}
\usepackage{multicol}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{url}

\renewcommand{\baselinestretch}{1.2}
\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\thesection\arabic{subsection}}

\newcolumntype{L}{>{$}l<{$}}
\setmainfont{Linux Libertine O}

\makeatletter
\def\old@comma{,}
\catcode`\,=13
\def,{%
  \ifmmode%
    \old@comma\discretionary{}{}{}%
  \else%
    \old@comma%
  \fi%
}
\makeatother

\title{Using Textual Features to Identify Document Reliability}
\author{Julien Cherry \and Patrick McGrath}
\date{April 26\textsuperscript{th}, 2019}

\begin{document}
	\maketitle
	\section{Introduction}

	In early 2018, the American Dialect Society named ``fake news'' its Word of the Year. They note that in 2016, ``its meaning was restricted to fictional or embellished stories presented as authentic news, disseminated for financial gain or for propagandistic purposes'', but that more recently, President Trump has often wielded it ``as a rhetorical bludgeon to disparage any news report that he happened to disagree with'' \cite{ads}. The term’s distinction as the Word of the Year and prevalence in political discourse highlights the seriousness of this issue in the present.

	Given the pervasiveness of fake news and other types of misinformation, people may have difficulty determining the reliability of articles they come across. We approach this problem using supervised learning to assess whether a user can trust a given article online.

	\section{Problem Formulation}

	We formulate the problem by starting with some dataset $D = \{\langle \text{title}_1, \text{author}_1, \text{text}_1, \text{reliability}_1 \rangle \ldots\}$ that consists of a set of tuples, each tuple representing an article. Each article has a title, author, full text, and a reliabiltiy judgment. From this dataset, we randomly sample a subset of the articles to get a training set $T_a = randomSample(D)$. Our testing set $T_b = D \setminus T_a$ is the set difference of the original dataset and the training set. We use the training set to train a classifier $C = trainClassifier(T_a)$. And finally, using a novel article $a_n \not\in D = \langle \text{title}_n, \text{author}_n, \text{text}_n \rangle$ (without a reliability judgment), the classifier determines a reliabiltiy prediction $p_n \in \{\text{reliable}, \text{unreliable}\} = predict(C, a_n)$ to report whether the article is likely to be reliable.

	\section{Methodology}

	To train our model, we used fake news data from data competiton platform Kaggle consisting of a 20,800-row CSV dataset for training and a 5,200-row CSV dataset for testing \cite{kaggle}. Each row represents a news article and contains a unique ID as well as the title, author, and text of the article. Additionally, the training file contains a reliability judgment label---1 for potentially unreliable, and 0 for reliable.

	Because we approached the problem with supervised learning and the testing dataset contains no reliability labels, we discarded it. We suspect that the reliability judgments for this dataset are merely unavailable in order to prevent competitors from training on the testing dataset.

	We extracted and cleaned 18,285 articles from the training dataset with the pandas library \cite{pandas} and then used a seed to pseudo-randomly withhold 50\% of the articles for use in a testing set. Using scikit-learn \cite{scikit-learn}, we vectorized the data into token--count tuples and weighted the features with TF--IDF to prioritize words that are more unique to a given document.

	We then used the data to train each of three classifiers: a Naïve Bayes classifier, a stochastic gradient descent classifier, and a passive aggressive classifier. Once the classifiers were trained, we used the model to predict the reliability of articles in the testing dataset.

	\subsection{Passive Aggressive Classification}

    Passive Aggressive Classification is a process using an online algorithm that works best when streaming data, in that the process uses data to update weights and then discards the data. Similar to other forms of classification, the objective is to classify objects into a series of buckets.

    To do this, we evaluate objects with a series of functions that then determine where in the classification space the object should land. If the classification implies that the weight value (which serves to separate different classification groups) is not correct, then a hinge loss function is used to update the weight.
    \break
    The basic algorithm for Passive Aggressive Classification is as follows:
    \break

	\def\BState{\State\hskip-\ALG@thistlm}
	\makeatother
	\begin{algorithm}
	\begin{algorithmic}[1]
			\Procedure{Passive Aggressive Classification}{}
			\State Initialize $w \gets (0, ..., 0)$
			\State Monitor stream
			\State Ingest new document $d \gets (d_1, ..., d_v)$
			\State Vectorize and apply TF--IDF
			\State Predict classification with $d^T w$
			\State Observe label for document
			\State Compute loss $L \gets max(0, 1 - y d^T w)$
			\State $w_{\text{new}} \gets w + y L d$
			\EndProcedure
		\end{algorithmic}
	\end{algorithm}


	\section{Empirical Results}

	\begin{tabular}{r | l l l}
		                & Naïve Bayes & Stochastic Gradient Descent & Passive Aggressive \\
		\hline
		accuracy        & 70.67\%     & 94.76\%                     & 96.31\%            \\
		false positives & 2           & 211                         & 150                \\
		false negatives & 2680        & 268                         & 187                \\
		specificity     & 33.72\%     & 93.49\%                     & 96.08\%            \\
		precision       & 99.92\%     & 96.67\%                     & 96.12\%            \\
	\end{tabular}
    \break
    
	\textit{Specificity} is calculated as the number of correct negative predictions divided by the total number of negatives.

	\textit{Precision} is the number of correct positive results divided by the number of positive results predicted by the classifier.

	\section{Analysis}

    When beginning the analysis of the findings attained from the three different classification methods we had to reorient ourselves around the objective, which is the identification of misinformation.
    
    If we defined the optimal approach as the one with the highest precision, that is, the highest accuracy rate when predicting reliable articles, then we would have chosen Naïve Bayes as the optimal classifier, given it boasts a precision of 99.92\%. We however thought that the optimal approach would be the one with the highest total accuracy, taking account to the correct classification of reliable and unreliable articles. Passive Aggressive was a clear winner here, with a general accuracy greater than the other two methods.
    
    After concluding that Passive Aggressive Classification was our optimal approach we began to look at the tokens that had both the highest and lowest weights.
    
    \subsection{Tokens with Lowest Weights}
	\begin{tabular}{r | l l}
		    & Weight        & Token         \\
		\hline
		1   & -0.077192     & said          \\
		2   & -0.077192	    & breitbart     \\
		3   & -0.071631     & twitter       \\
		4   & -0.063136	    & 2017          \\
		5   & -0.056375	    & follow        \\
		6   & -0.055649	    & mr            \\
		7   & -0.051476	    & ms            \\
		8   & -0.046095	    & added         \\
		9   & -0.035301	    & president     \\
		10  & -0.034659     & mrs           \\
	\end{tabular}

    \subsection{Tokens with Highest Weights}
	\begin{tabular}{r | l l}
		    & Weight            & Token         \\
		\hline
		1   & 0.072100	        & anti          \\
		2   & 0.063116		    & hillary       \\
		3   & 0.060869          & October       \\
		4   & 0.056147	        & November      \\
		5   & 0.055459	        & non           \\
		6   & 0.048166	        & co            \\
		7   & 0.047127	        & 2016          \\
		8   & 0.046667	        & us            \\
		9   & 0.044610	        & self          \\
		10  & 0.041281          & elect         \\
	\end{tabular}

	\break

    After looking at the token weight pairs that were most influential in determining whether an article was reliable or not, we came to the realization that generally speaking the context of the word is completely lost with our approach, which decreases our ability to truly identify whether a body of text is misinformation or not.

    We also noticed clear bias in the findings, which is likely a result of the dataset that is being operated on. For example, the term 2017 has a large negative weight associated to it, but the term 2016 has a large positive weight associated to it. It does not seem logical that a date would be a determining factor in whether or not a body of text is misinformation or not, and therefore we came to the conclusion that this was an example of over-fitting on the training data. In this case, it is clear that correlation does not imply causation.

	\section{Future Work}

	Work in this area could be used to inform social media users whether linked articles are likely to be reliable or not. However, its implementation need be informed---Pennycook et al. found that labelling disputed articles with a warning causes unlabelled articles to appear more accurate. Crucially, they found that this \textit{Implied Truth Effect} is reversed when verified articles are also labelled \cite{pennycook}.

	\begin{itemize}
		\item Investigate what kinds of articles are misclassified
		\item Tweak specificity and precision of classifier
		\item Use n-grams, Word2Vec, Doc2Vec
		\begin{itemize}
			\item Improve context around words, instead of standalone words
		\end{itemize}
	\end{itemize}

	\section{Reflection}

	\begin{itemize}
		\item Have a more established definition of what misinformation is
		\item Opportunity to debug classification process
		\begin{itemize}
			\item Identify improvements
			\item Adjust features
		\end{itemize}
	\end{itemize}

	\bibliographystyle{plainnat}
	\bibliography{references}
\end{document}
